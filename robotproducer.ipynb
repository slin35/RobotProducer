{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "robotproducer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNPxkJDtUHQa6KbPZs/Jbn4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slin35/RobotProducer/blob/main/robotproducer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Xl421vr3k0"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9S_Gmc_sG3y"
      },
      "source": [
        "# import data\n",
        "data1 = pd.read_csv('/tmdb_5000_movies.csv')\n",
        "data2 = pd.read_csv('/tmdb_5000_credits.csv')\n",
        "\n",
        "data1 = data1[['genres', 'title', 'overview', 'id']]\n",
        "data2 = data2[['movie_id', 'title', 'cast', 'crew']]\n",
        "\n",
        "data = pd.merge(data1, data2, left_on='id', right_on='movie_id', how='inner')\n",
        "data = data[['title_x', 'overview', 'genres', 'cast', 'crew']]\n",
        "data.rename({'title_x': 'title'}, axis=1, inplace=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7vh4BF4vEnc"
      },
      "source": [
        "# cleanup genres\n",
        "data['genres'] = [json.loads(i) if i != [] else [] for i in data['genres']]\n",
        "data['genres'] = [[j['name'] if 'name' in j else [] for j in i] for i in data['genres']]\n",
        "\n",
        "# cleanup cast\n",
        "data['cast'] = [json.loads(i) if i != [] else [] for i in data['cast']]\n",
        "data['cast'] = [[j['name'] if 'name' in j else [] for j in i] for i in data['cast']]\n",
        "\n",
        "# cleanup crew\n",
        "data['crew'] = [json.loads(i) if i != [] else [] for i in data['crew']]\n",
        "data['crew'] = [[j['name']  for j in i if 'job' in j and j['job'] == 'Director'] for i in data['crew']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9EdHouCR6Hw",
        "outputId": "46b9bfaf-8b6d-4972-86c5-d8cbafaee196"
      },
      "source": [
        "# tokenize overview and remove stopwords and punctuation\n",
        "'''\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "data['overview'] = [word_tokenize(line) if type(line) is str else [] for line in data['overview']]\n",
        "\n",
        "data['overview'] = [[w for w in line if w not in stop_words and w not in string.punctuation] for line in data['overview']]\n",
        "\n",
        "# lemmatize words\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "data['overview'] = [[lemmatizer.lemmatize(w) for w in line] for line in data['overview']]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aet0E_nXrjj6",
        "outputId": "e49a8dac-4740-4d15-8f4f-250c083424d2"
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St4maUdQD931"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywQFc4I55QCP",
        "outputId": "653bd35c-c3cd-4e8f-b7c1-3628eda76da1"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_iaMPu47NCd"
      },
      "source": [
        "import bert\n",
        "\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import bert_tokenization\n",
        "from bert import modeling\n",
        "from bert import run_classifier_with_tfhub"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWBnJP409tYr",
        "outputId": "e22076af-820f-41d8-d265-f9907a31224c"
      },
      "source": [
        "# BERT-based, uncased model: 12-layer, 768-hidden, 12-heads, 110M parematers\n",
        "\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])    \n",
        "  return bert_tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gmffLgMJjYX",
        "outputId": "9020c37e-2eb1-4224-ba3a-010f1180fa3b"
      },
      "source": [
        "tokenizer.tokenize(\"this is my example\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'my', 'example']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}